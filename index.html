<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta charset="utf-8" />
  <title property="foaf:name schema:name">Towards Cost-model-based Query Execution over Hybrid Linked Data Fragments Interfaces</title>
  <link rel="stylesheet" media="screen" href="styles/screen.css" />
  <link rel="stylesheet" media="print"  href="styles/print.css" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  
  <meta name="citation_title" content="Towards Cost-model-based Query Execution over Hybrid Linked Data Fragments Interfaces">
  <meta name="citation_author" content="Amr Azzam" />
  <meta name="citation_author" content="Ruben Taelman" />
  
  <meta name="citation_publication_date" content="2019/12/12" />
</head>

<body prefix="dctypes: http://purl.org/dc/dcmitype/ pimspace: http://www.w3.org/ns/pim/space# rsa: http://www.w3.org/ns/auth/rsa# cert: http://www.w3.org/ns/auth/cert# wgs: http://www.w3.org/2003/01/geo/wgs84_pos# biblio: http://purl.org/net/biblio# bibo: http://purl.org/ontology/bibo/ book: http://purl.org/NET/book/vocab# ov: http://open.vocab.org/terms/ doap: http://usefulinc.com/ns/doap# dbr: http://dbpedia.org/resource/ dbp: http://dbpedia.org/property/ sio: http://semanticscience.org/resource/ opmw: http://www.opmw.org/ontology/ deo: http://purl.org/spar/deo/ doco: http://purl.org/spar/doco/ cito: http://purl.org/spar/cito/ fabio: http://purl.org/spar/fabio/ solid: http://www.w3.org/ns/solid/terms# acl: http://www.w3.org/ns/auth/acl# dio: https://w3id.org/dio# lsc: http://linkedscience.org/lsc/ns#" typeof="schema:CreativeWork sioc:Post prov:Entity lsc:Research">
  <header>
  <h1 id="towards-cost-model-based-query-execution-over-hybrid-linked-data-fragments-interfaces">Towards Cost-model-based Query Execution over Hybrid Linked Data Fragments Interfaces</h1>

  <ul id="authors">
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="#" typeof="foaf:Person schema:Person" resource="#">Amr Azzam</a><a href="#wu"><sup>1</sup></a></li>
    <li><a rev="lsc:participatesIn" property="foaf:maker schema:creator schema:author schema:publisher" href="http://www.rubensworks.net/" typeof="foaf:Person schema:Person" resource="http://www.rubensworks.net/#me">Ruben Taelman</a><a href="#idlab"><sup>2</sup></a></li>
  </ul>

  <ul id="affiliations">
    <li id="wu"><sup>1</sup>Vienna University of Economics and Business,
          Vienna, Austria,
          {firstname.lastname}@wu.ac.at</li>
    <li id="idlab"><sup>2</sup>IDLab,
          Department of Electronics and Information Systems,
          Ghent University – imec,
          {firstname.lastname}@ugent.be</li>
  </ul>

  <section id="abstract" inlist="" rel="schema:hasPart" resource="#abstract">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Abstract</h2>
      <!-- Context      -->
      <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit.
<!-- Need         -->
Vestibulum finibus dignissim augue, id pellentesque est facilisis non.
<!-- Task         -->
Donec fringilla dolor non neque iaculis blandit.
<!-- Object       -->
Praesent aliquet eleifend iaculis.
<!-- Findings     -->
Quisque pellentesque at odio ac bibendum.
<!-- Conclusion   -->
Pellentesque imperdiet felis urna, quis facilisis lacus gravida non.
<!-- Perspectives -->
Donec quis lectus eget sem tempor tristique pellentesque in dolor.</p>

    </div>
</section>

</header>

<main>
  <section id="introduction" inlist="" rel="schema:hasPart" resource="#introduction">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Introduction</h2>

      <p>Linked Data (LD) provides standards for publishing (RDF)and (SPARQL) querying Knowledge Graphs on the Web,serving, accessing and processing such open, decentralized datasets is often practically impossible due to query timeouts on publicly available SPARQL endpoints show. Another option is to make the data publicly available as data dumps. In order to query the datadumps, the full dataset needs to be downloaded on the local machine that supports SPARQL query engine.  These could be effective option with very huge SPARQL query workload, but it does not comply to the aim of Semantic Web which allows the live querying.</p>

      <p>Alternatively Linked Data Fragments (LDF), introduced the foundation of a framework that distributes the load of the query execution between the clients and the server as well as the financial costs of queries execution among data providers.</p>

      <p>First, LDF laid the basis to define Triple Pattern Fragments (TPF) interface that attempts to tackle the problem of availability through providing intelligent TPF clients  which shifted the complex query processing workload to the client side, but with the cost of the increase of network overhead due to unnecessary transfer of large intermediate results in addition to longer query execution time that lowers the performance.</p>

      <p>To address the drawbacks of TPF, Bindings-Restricted Triple Pattern Fragments (brTPF)  is an extended interface of TPF that gives a slight boost to the performance of 	query execution through by attaching intermediate results to triple pattern requests together with distributing the join between the client and the server using the bind join strategy. In this manner, brTPF reduces the number of HTTP requests in addition to minimize the amount of data transferred than the original TPF. However, brTPF still have considerably high HTTP requests besides the shortcoming of the ability to scale with larger datasets.</p>

      <p>Recently, SaGe is a Web preemption based SPARQL query engine designed to avoid the starvation of the simple queries waiting for the complex ones that consume the server resources. SaGe utilized Round-Robin algorithm to maintain a fair allocation of server resources between queries. SaGe formalized a model that enables to suspend and proceed queries with the mechanism to save the state of the query execution. Additionally, SaGe has implemented full mappings operators such as ORDER BY, OPTIONAL as well as aggregation function to be executed on the client-side. Experiments showed that SaGe has enhanced the average completion time per client in addition to reduces the average network traffic per client. However, SaGe extensively consumes the server resources. Besides, the performance of SaGe is degrading with the increasing sizes of the knowledge graphs as well as the execution time of the complex queries is substantially increased.</p>

      <p>Finally, smartKG is a novel approach that introduced a new paradigm to distribute the query processing between the client and the server through combining shipping compressed knowledge graph partitions in addition to intermediate results shipped using TPF. The experimental evaluation demonstrated that smartKG outperformed the existing approaches in server resource usage in addition to the average workload execution time as well as less timeout queries. On the other hand, SPARQL endpoints and SaGe has a better performance than smartKG with less number of clients and small-scale knowledge graphs. Although smart-KG has better average workload execution time, TPF and SaGe outperform smartKG in certain types of queries.</p>

      <p>The main objective of this research is to provide a more intelligent interface that benefits from the variety of capabilities of the currently existing query engine through introducing double-side (client/server) cost models. Thus the proposed interface will insure that the server does not exceed its processing capabilities while maintaining a high query run time performance.</p>

      <p class="todo">Definitely cite these papers:
* LDF+TPF, SmartKG(?), Sage
* <a href="https://cpb-ap-se2.wpmucdn.com/blogs.auckland.ac.nz/dist/b/412/files/2019/10/paper_421.pdf">Towards More Intelligent SPARQL Querying Interfaces</a>
    Problem: No concrete proposals
* <a href="https://link.springer.com/chapter/10.1007/978-3-319-93417-4_26">Intelligent Clients for Replicated Triple Pattern Fragments</a>
    Problem: Only for replicated TPF interfaces, not combining different LDFs.
* Comunica</p>

      <p class="todo">problem: different interfaces have pros/cons -&gt; there’s no way of combining them (based on params, load, …)
solution: 2 cost-models: server and client</p>

    </div>
</section>

  <section id="solution" inlist="" rel="schema:hasPart" resource="#solution">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Hybrid Framework</h2>

      <p class="todo">Shorten</p>

      <p>The goal of our proposed framework
is to allow servers to expose different kinds of interfaces
based on the current server load and the used SPARQL queries.
Instead of making the server allow <em>just one interface</em> type per query,
we propose allowing a <em>collection of interfaces</em> to be exposed per query.
This allows the client to select the most desired interface
based on the clients capabilities and circumstances.</p>

      <p>To achieve such a hybrid of server interfaces,
we make use of a server-side cost model for selecting a set of interfaces based on a given query,
and a client-side cost model for determining a query execution plan based on the allowed interfaces.
<a href="#figure-solution">Fig. 1</a> shows an overview of this framework
where client-side query engines start by sending a query <code>q</code> to the server,
and receive an answer that contains a token <code>t</code> and a set of allowed interfaces <code>I</code>.
Based on the returned interfaces,
the client can determine a query decomposition over these interfaces.
These (sub)queries can then be resolved by requesting the appropriate interfaces using the given token.
Hereafter, we explain the server and client processes in more detail.</p>

      <figure id="figure-solution">
<img src="img/hybrid-querying.svg" alt="[Hybrid Linked Data Fragments]" />
<figcaption>
          <p><span class="label">Fig. 1:</span> Overview of client-server communication for a cost-model-based query execution over a hybrid of Linked Data Fragments interfaces.</p>
        </figcaption>
</figure>

      <h3 id="server-component">Server Component</h3>

      <p>The server component of our framework consists of two elements:</p>

      <ol>
        <li>A cost model for calculating a set of allowed interfaces based on a given query and a set of internal metrics.</li>
        <li>A token-based wrapper over a set of existing LDF interfaces.</li>
      </ol>

      <p>We explain these two elements hereafter.</p>

      <h4 id="cost-model">Cost Model</h4>

      <p>The goal of this server-side cost model is two-fold:</p>

      <ol>
        <li>The availability of the server must be ensured.</li>
        <li>Queries must be executed as fast as possible.</li>
      </ol>

      <p>Since the second goal can sometimes be detrimental to the server availability,
for example when many concurrent users are sending highly complex queries,
the first goal must always have priority in the model.</p>

      <p>Based on these goals, the model should be able to make a suggestion for a set of interfaces
based on a given query and a set of internal metrics.
For this, we propose a set of internal metrics such as the current CPU usage, memory usage and network I/O.
The server administrator must be able to configure an upper limit for these metrics,
so that the cost model can select interfaces that optimize both goals.</p>

      <p><a href="#algorithm-get-allowed-interfaces">Listing 1</a> shows the pseudocode of an algorithm
that can be used to calculate a set of allowed interfaces.
For each incoming query <code>q</code>,
the algorithm iterates over all available interfaces, and all metrics.
For each metric, the expected metric value increase is calculated
for the given query using <code>CalculateMetricIncrease(q, metric)</code>.
If when adding this value to the current metric’s value does not exceed the maximum allowed metric value,
then the loop continues.
If all metrics pass for a given interface,
then an interface is considered an <em>allowed interface</em>.</p>

      <figure id="algorithm-get-allowed-interfaces" class="listing">
<pre><code>FUNCTION GetAllowedInterfaces(q, metrics, interfaces, MetricsCurrent, MetricsMax)
</code><code>  INPUT:
</code><code>    q: a SPARQL query
</code><code>    metrics: a set of metrics applicable for the cost model
</code><code>    interfaces: a set of available interfaces
</code><code>    MetricsCurrent: function to get the current value of a metric
</code><code>    MetricsMax: function to get the maximum allowed value of a metric
</code><code>  OUTPUT:
</code><code>    set of allowed interfaces
</code><code>allowedInterfaces = []
</code><code>FOREACH interface IN interfaces
</code><code>  validInterface = true
</code><code>  FOREACH metric IN metrics
</code><code>    increase = CalculateMetricIncrease(q, interface)
</code><code>    IF MetricsCurrent(metric) + increase &gt; MetricsMax(metric)
</code><code>      validInterface = false
</code><code>  IF validInterface
</code><code>    allowedInterfaces.push(validInterface)
</code><code>RETURN allowedInterfaces</code></pre>
<figcaption>
          <p><span class="label">Listing 1:</span> Algorithm for calculating the allowed interfaces for a given query.</p>
        </figcaption>
</figure>

      <p>Based on our algorithm, the <code>CalculateMetricIncrease</code> still needs a concrete implementation.
For this, different possibilities exist.
For instance, heuristics for query complexity can be used to estimate metric value increases,
such as query string length, the depth of the basic graph patterns or the used query operators.
Furthermore, other implementations may be based on query log analysis,
where models could be based on machine learning techniques.</p>

      <p class="todo">Add citations for query complexity estimations, logs, machine learning</p>

      <h4 id="interface-wrapper">Interface Wrapper</h4>

      <p>Based on the server-side cost model,
the server can wrap over a number of LDF interfaces
that the publisher wants to expose.
This wrapper is a proxy that accepts SPARQL queries,
and replies with a token and a set of allowed interfaces
that have been calculated for the given query using the server-side cost model.
The token is <em>required</em> for performing any requests to any of the wrapped LDF interfaces.</p>

      <p>This token should be seen as temporary <em>permission</em>
to make use of a specific set of query capabilities from the data publisher.
It is important that the server validates this token upon every request to an LDF interface.
If the server would not do this,
a client could simply ignore the set of allowed interfaces,
and always execute queries against the most expressive interface (e.g. SPARQL endpoint),
even if this interface was not allowed by the server.</p>

      <p>Optionally, the server could keep track of token usages
to check whether or not the client does indeed use it
to execute the query it got permission for, and nothing more.
Since keeping track of this token usage could require significant server effort,
simpler heuristics could be used,
such as limiting the temporal validity of a token to the estimated execution time.</p>

      <p>An optional enhancement of the server could be to directly
reply with a SPARQL query response
if the only allowed server was a SPARQL endpoint,
because the client will be likely to make such a subsequent request.</p>

      <h3 id="client-component">Client Component</h3>

      <p>In most cases, the primary goal of clients is to execute queries as fast as possible,
either in overal execution time,
or in continuous efficiency.
There could however be a number of metrics that can soften this need for fast query execution,
such as reducing CPU or bandwidth usage.</p>

      <p class="todo">Cite Maribel’s diefficiency for continuous efficiency.</p>

      <p>Using the server-side hybrid of LDF interfaces that was explained before,
clients will retrieve a set of allowed interfaces based on a given query.
Based on these interfaces, the client should determine a query plan that makes use of the interfaces
in such a way that is as efficient as possible with respect to the client’s metrics.
While most client-side query algorithms focus on splitting up queries for execution against a single type of interface,
new algorithms could be designed for this that <em>intelligently combine interfaces</em> for certain subqueries.</p>

      <p>Next to client metrics, there could be additional parameters that could influence
the selection of interfaces to query from.
For example, if the client knows beforehand that it will have to execute <em>many</em> queries against the same dataset,
then it might be more efficient to download the full dump of the dataset,
even if a SPARQL endpoint was allowed for the initial query.
Another case that can influence interface selection
would be when certain partial dumps of the dataset are already available locally,
or within a network of peers.</p>

      <p class="todo">Cite P2P querying within browsers</p>

    </div>
</section>

  <section id="conclusions" inlist="" rel="schema:hasPart" resource="#conclusions">
<div datatype="rdf:HTML" property="schema:description">
      <h2 property="schema:name">Conclusions</h2>

      <p class="todo">show our plan for implementation (with Comunica client)</p>

      <p class="todo">comunica-like software packet for installing server components, instead of installing everything one-by-one manually. to hide the complexity for publishers.</p>

      <p class="todo">plans for evaluation, perspectives</p>

    </div>
</section>

</main>

<footer></footer>

</body>
</html>
